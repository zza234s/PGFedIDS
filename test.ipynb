{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "samples = [(0, 36188), (1, 5174), (2, 1312), (3, 968), (4, 962), (5, 628), (6, 562)]\n",
    "\n",
    "labels = ['Normal', 'Bot', 'Infilteration', 'DDos', 'DoS', 'Web', 'Brute-force']\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_samples = sum([count for _, count in samples])\n",
    "# Calculate the proportion of each class\n",
    "proportions = [(cls, count / total_samples) for cls, count in samples]\n",
    "\n",
    "print(proportions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:13:42.572657Z",
     "start_time": "2024-06-28T14:13:40.720912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "# NLLLoss+LogSoftmax\n",
    "# logsoftmax=log(softmax(x))\n",
    "m = nn.LogSoftmax(dim=1) #横向计算\n",
    "loss = nn.NLLLoss()\n",
    "# 3行5列的输入，即3个样本各包含5个特征，每个样本通过softmax产生5个输出\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "#这里的104就是标签，即input中的第一行的识别出来的标签为1，第二行的标签为0\n",
    "target = torch.tensor([1, 0, 4]) \n",
    "# NLL将取输出矩阵中第0行的第1列、第1行的第0列、第2行的第4列加负号求和\n",
    "output = loss(m(input), target)\n"
   ],
   "id": "6b06aa084688b615",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:13:49.648143Z",
     "start_time": "2024-06-28T14:13:49.634069Z"
    }
   },
   "cell_type": "code",
   "source": "input",
   "id": "e41e3bd801c76b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5614,  0.2378,  0.4337, -0.0815, -0.8311],\n",
       "        [-0.9018,  1.1236,  1.2969, -0.4175,  0.6018],\n",
       "        [ 1.7519,  1.1706,  1.1448, -0.9843,  2.2610]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:13:58.910098Z",
     "start_time": "2024-06-28T14:13:58.891011Z"
    }
   },
   "cell_type": "code",
   "source": "m(input)",
   "id": "1fee62f88875f51f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2173, -1.5408, -1.3450, -1.8601, -2.6098],\n",
       "        [-3.1661, -1.1406, -0.9673, -2.6818, -1.6625],\n",
       "        [-1.3436, -1.9248, -1.9507, -4.0798, -0.8345]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:41:36.255163Z",
     "start_time": "2024-07-05T01:41:36.175892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from pytorch_model_summary import summary"
   ],
   "id": "754237ce8825a8c6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:41:46.343933Z",
     "start_time": "2024-07-05T01:41:46.317932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GCE(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, dev='cpu'):\n",
    "        super(GCE, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(num_classes, in_features)\n",
    "        self.dev = dev\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        embeddings = self.embedding(torch.tensor(range(self.num_classes), device=self.dev))\n",
    "        cosine = F.linear(F.normalize(x), F.normalize(embeddings))\n",
    "        one_hot = torch.zeros(cosine.size(), device=self.dev)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "\n",
    "        softmax_value = F.log_softmax(cosine, dim=1)\n",
    "        softmax_loss = one_hot * softmax_value\n",
    "        softmax_loss = - torch.mean(torch.sum(softmax_loss, dim=1))\n",
    "\n",
    "        return softmax_loss\n",
    "\n",
    "\n",
    "class CoV(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(CoV, self).__init__()\n",
    "\n",
    "        self.Conditional_gamma = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([in_dim]),\n",
    "        )\n",
    "        self.Conditional_beta = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([in_dim]),\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        gamma = self.Conditional_gamma(context)\n",
    "        beta = self.Conditional_beta(context)\n",
    "\n",
    "        out = torch.multiply(x, gamma + 1)\n",
    "        out = torch.add(out, beta)\n",
    "        out = self.act(out)\n",
    "        return out"
   ],
   "id": "3a45b9c4e0d0b8a1",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:46:19.255382Z",
     "start_time": "2024-07-05T01:46:19.223383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GCE = GCE(in_features=80,num_classes=7)\n",
    "CoV = CoV(80)"
   ],
   "id": "1c798c840947af0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:46:44.172159Z",
     "start_time": "2024-07-05T01:46:43.662372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_tensor = torch.rand(1024,80)\n",
    "model_summary=summary(GCE, input_tensor)"
   ],
   "id": "f5a3030b7026f99f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1024\u001B[39m,\u001B[38;5;241m80\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m model_summary\u001B[38;5;241m=\u001B[39m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mGCE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\torch231_pflib\\lib\\site-packages\\pytorch_model_summary\\model_summary.py:118\u001B[0m, in \u001B[0;36msummary\u001B[1;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001B[0m\n\u001B[0;32m    115\u001B[0m model_training \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtraining\n\u001B[0;32m    117\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m--> 118\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_training:\n\u001B[0;32m    121\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\torch231_pflib\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\torch231_pflib\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "43800d542d142410"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:29:38.103004Z",
     "start_time": "2024-07-03T10:29:38.090004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CICIDS_Transformer(nn.Module):\n",
    "    def __init__(self, in_feature=80,out_features=7,d_model=80,nhead=2, dim_feedforward=512, num_layers=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(in_feature, out_features)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        # x = self.fc1(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "80507df79da81780",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:29:51.656800Z",
     "start_time": "2024-07-03T10:29:51.599803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CICIDS_Transformer(in_feature=122, out_features=5, d_model=122)\n",
    "input_tensor = torch.rand(1024,122)\n",
    "model_summary=summary(model, input_tensor)\n",
    "print(model_summary)"
   ],
   "id": "42ba648529f72785",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "   TransformerEncoder-1         [1024, 122]         930,370         930,370\n",
      "               Linear-2           [1024, 5]             615             615\n",
      "============================================================================\n",
      "Total params: 930,985\n",
      "Trainable params: 930,985\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:31:03.369859Z",
     "start_time": "2024-07-03T10:31:03.360860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = nn.Linear(80,7, bias=True)\n",
    "input_tensor = torch.rand(1024,80)\n",
    "model_summary=summary(model, input_tensor)\n",
    "print(model_summary)"
   ],
   "id": "ada7f6944a9d0bf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1           [1024, 7]             567             567\n",
      "=======================================================================\n",
      "Total params: 567\n",
      "Trainable params: 567\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:26:51.791368Z",
     "start_time": "2024-07-03T10:26:51.785366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = CICIDS_Transformer(in_feature=122, out_features=5, d_model=122)\n",
    "model= nn.Linear(80,7,bias=True)"
   ],
   "id": "ea9071c480df234",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:26:52.182018Z",
     "start_time": "2024-07-03T10:26:52.149864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = torch.randn(1024,80)  # 随机生成一个输入张量，这个尺寸应该与模型输入的尺寸相匹配\n",
    "flops, params = profile(model, inputs=(input,))\n",
    "\n",
    "# 将结果转换为更易于阅读的格式\n",
    "flops, params = clever_format([flops, params], '%.3f')"
   ],
   "id": "7515aef6db169ba7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T10:26:53.049533Z",
     "start_time": "2024-07-03T10:26:53.032531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(f\"运算量：{flops}, 参数量：{params}\")"
   ],
   "id": "dcf43533ec739f19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运算量：573.440K, 参数量：0.000B\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T11:32:59.511972Z",
     "start_time": "2024-06-29T11:32:59.498974Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "21c25b9931879c35",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77dd7f471b539347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
